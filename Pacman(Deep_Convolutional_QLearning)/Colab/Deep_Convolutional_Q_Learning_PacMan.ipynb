{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EBayego/AI_Gymnasium/blob/main/Pacman(Deep_Convolutional_QLearning)/Colab/Deep_Convolutional_Q_Learning_PacMan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Convolutional Q-Learning for Pac-Man"
      ],
      "metadata": {
        "id": "EAiHVEoWHy_D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 0 - Installing the required packages and importing the libraries"
      ],
      "metadata": {
        "id": "tjO1aK3Ddjs5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing Gymnasium"
      ],
      "metadata": {
        "id": "NwdRB-ZLdrAV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbnq3XpoKa_7"
      },
      "outputs": [],
      "source": [
        "!pip install gymnasium\n",
        "!pip install \"gymnasium[atari, accept-rom-license]\"\n",
        "!apt-get install -y swig\n",
        "!pip install gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing the libraries"
      ],
      "metadata": {
        "id": "H-wes4LZdxdd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ho_25-9_9qnu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from collections import deque\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1 - Building the AI"
      ],
      "metadata": {
        "id": "m7wa0ft8e3M_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating the architecture of the Neural Network"
      ],
      "metadata": {
        "id": "dlYVpVdHe-i6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(nn.Module):\n",
        "  def __init__(self, action_size, seed = 42):\n",
        "    super(Network, self).__init__()\n",
        "    self.seed = torch.manual_seed(seed)\n",
        "    self.conv1 = nn.Conv2d(3, 32, kernel_size=8, stride=4) #convolutional layer, 3 initial input channles that equals to RGB, and the other parameters are for experimentation\n",
        "    self.bn1 = nn.BatchNorm2d(32) # normalization layers\n",
        "    self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
        "    self.bn2 = nn.BatchNorm2d(64)\n",
        "    self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
        "    self.bn3 = nn.BatchNorm2d(64)\n",
        "    self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=1)\n",
        "    self.bn4 = nn.BatchNorm2d(128)\n",
        "\n",
        "    self.fc1 = nn.Linear(10*10*128, 512) #dimensions*output_features from the convolutional layer\n",
        "    self.fc2 = nn.Linear(512, 256)\n",
        "    self.fc3 = nn.Linear(256, action_size)\n",
        "\n",
        "  def forward(self, state):\n",
        "    x = F.relu(self.bn1(self.conv1(state)))\n",
        "    x = F.relu(self.bn2(self.conv2(x)))\n",
        "    x = F.relu(self.bn3(self.conv3(x)))\n",
        "    x = F.relu(self.bn4(self.conv4(x)))\n",
        "\n",
        "    x = x.view(x.size(0), -1) #flattening layer\n",
        "\n",
        "    x = F.relu(self.fc1(x)) #sends the info throw the connected layers having the ReLU activation function applied\n",
        "    x = F.relu(self.fc2(x))\n",
        "    return self.fc3(x)\n"
      ],
      "metadata": {
        "id": "Wd9jZmB3lfFh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2 - Training the AI"
      ],
      "metadata": {
        "id": "rUvCfE_mhwo2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up the environment"
      ],
      "metadata": {
        "id": "WWCDPF22lkwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "env = gym.make('MsPacmanDeterministic-v0', full_action_space = False)\n",
        "state_shape = env.observation_space.shape #dimensions of the observation space (ex. two-dimensional = (4, ), three-dimensional = (3,3,3))\n",
        "state_size = env.observation_space.shape[0] #total size of the observation space (ex. two-dimensional = 4, three-dimensional = 27)\n",
        "num_actions = env.action_space.n #number of possible actions\n",
        "print('State shape: ', state_shape)\n",
        "print('State size: ', state_size) #delete?\n",
        "print('Number of actions: ', num_actions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikrII2kohfIp",
        "outputId": "3cd7c732-48f0-45f5-eb34-4b2a964ca9b9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001b[33mWARN: The environment MsPacmanDeterministic-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
            "  logger.deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State shape:  (210, 160, 3)\n",
            "State size:  210\n",
            "Number of actions:  9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initializing the hyperparameters"
      ],
      "metadata": {
        "id": "Bx6IdX3ciDqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 5e-4 #inicial weight\n",
        "minibatch_size = 64 #number of observations used in one step of the training to update the model parameters\n",
        "discount_factor = 0.99 #number that gives value to the rewards obtained after performing certain actions: a low number ~0 focuses on immediate rewards, a high number ~1 focuses on long-term rewards."
      ],
      "metadata": {
        "id": "4eu5WxMSisfi"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing the frames"
      ],
      "metadata": {
        "id": "U2bDShIEkA5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "def preprocess_frame(frame):\n",
        "  frame = Image.fromarray(frame)\n",
        "  preprocess = transforms.Compose([transforms.Resize((128, 128)), transforms.ToTensor()])\n",
        "  return preprocess(frame).unsqueeze(0)"
      ],
      "metadata": {
        "id": "z-8Xz_N5ocny"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementing the DCQN class"
      ],
      "metadata": {
        "id": "imMdSO-HAWra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent():\n",
        "  def __init__(self, action_size):\n",
        "    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.action_size = action_size\n",
        "    self.local_qnetwork = Network(action_size).to(self.device) #used to select actions\n",
        "    self.target_qnetwork = Network(action_size).to(self.device) #used to calculate the Q values of future actions, that will be used in the local network\n",
        "    self.optimizer = optim.Adam(self.local_qnetwork.parameters(), lr = learning_rate)\n",
        "    self.memory = deque(maxlen = 10000)\n",
        "\n",
        "  def step(self, state, action, reward, next_state, done):\n",
        "    state = preprocess_frame(state)\n",
        "    next_state = preprocess_frame(next_state)\n",
        "    self.memory.append((state, action, reward, next_state, done))\n",
        "    if len(self.memory) > minibatch_size: #learn from all the batch of experiences\n",
        "      experiences = random.sample(self.memory, k = minibatch_size)\n",
        "      self.learn(experiences, discount_factor)\n",
        "\n",
        "  def act(self, state, epsilon = 0.): #epsilon is the greedy factor, the probability of performing a random action instead of the optimal one\n",
        "    state = preprocess_frame(state).to(self.device)\n",
        "    self.local_qnetwork.eval()\n",
        "    with torch.no_grad():\n",
        "      action_values = self.local_qnetwork(state) #Q values of every possible action now\n",
        "    self.local_qnetwork.train()\n",
        "    if random.random() > epsilon:\n",
        "      return np.argmax(action_values.cpu().data.numpy())\n",
        "    else:\n",
        "      return random.choice(np.arange(self.action_size))\n",
        "\n",
        "  def learn(self, experiences, discount_factor):\n",
        "    states, actions, rewards, next_states, dones = zip(*experiences)\n",
        "    states = torch.from_numpy(np.vstack(states)).float().to(self.device) #takes the state of all experience tuples, and converts them into tensors\n",
        "    actions = torch.from_numpy(np.vstack(actions)).long().to(self.device)\n",
        "    rewards = torch.from_numpy(np.vstack(rewards)).float().to(self.device)\n",
        "    next_states = torch.from_numpy(np.vstack(next_states)).float().to(self.device)\n",
        "    dones = torch.from_numpy(np.vstack(dones)).to(dtype=torch.uint8).float().to(self.device)\n",
        "    next_qtargets = self.target_qnetwork(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "    q_targets = rewards + (discount_factor * next_qtargets * (1 - dones))\n",
        "    q_expected = self.local_qnetwork(states).gather(1, actions)\n",
        "    loss = F.mse_loss(q_targets, q_expected)\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "\n",
        "  # soft update doesn't improve this specific model (tested in experimentation)"
      ],
      "metadata": {
        "id": "KlCDEAORpV6f"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initializing the DCQN agent"
      ],
      "metadata": {
        "id": "yUg95iBpAwII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent(num_actions)"
      ],
      "metadata": {
        "id": "tWhi4VHNssBc"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the DCQN agent"
      ],
      "metadata": {
        "id": "CK6Zt_gNmHvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_episodes = 2000\n",
        "max_episode_timesteps = 10000\n",
        "initial_epsilon = 1.0\n",
        "final_epsilon = 0.01\n",
        "epsilon_decay = 0.995\n",
        "epsilon = initial_epsilon\n",
        "scores = deque(maxlen = 100)\n",
        "\n",
        "for episode in range(1, num_episodes + 1):\n",
        "  state, _ = env.reset()\n",
        "  score = 0\n",
        "  for t in range(max_episode_timesteps):\n",
        "    action = agent.act(state, epsilon)\n",
        "    next_state, reward, done, _, _ = env.step(action)\n",
        "    agent.step(state, action, reward, next_state, done)\n",
        "    state = next_state\n",
        "    score += reward\n",
        "    if done:\n",
        "      break\n",
        "  scores.append(score)\n",
        "  epsilon = max(final_epsilon, epsilon_decay * epsilon)\n",
        "  print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores)), end = \"\")\n",
        "  if episode % 100 == 0:\n",
        "    print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores)))\n",
        "  if np.mean(scores) >= 500.0:\n",
        "    print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(episode - 100, np.mean(scores)), end = \"\")\n",
        "    torch.save(agent.local_qnetwork.state_dict(), 'checkpoint.pth')\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIUGdYqAssg7",
        "outputId": "384d7fad-d19b-4b66-cb66-3c6c420395d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 88\tAverage Score: 283.64"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3 - Visualizing the results"
      ],
      "metadata": {
        "id": "-0WhhBV8nQdf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cb9nVvU2Okhk"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import io\n",
        "import base64\n",
        "import imageio\n",
        "from IPython.display import HTML, display\n",
        "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
        "\n",
        "def show_video_of_model(agent, env_name):\n",
        "    env = gym.make(env_name, render_mode='rgb_array')\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    frames = []\n",
        "    while not done:\n",
        "        frame = env.render()\n",
        "        frames.append(frame)\n",
        "        action = agent.act(state)\n",
        "        state, reward, done, _, _ = env.step(action)\n",
        "    env.close()\n",
        "    imageio.mimsave('video.mp4', frames, fps=30)\n",
        "\n",
        "show_video_of_model(agent, 'MsPacmanDeterministic-v0')\n",
        "\n",
        "def show_video():\n",
        "    mp4list = glob.glob('*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "    else:\n",
        "        print(\"Could not find video\")\n",
        "\n",
        "show_video()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}